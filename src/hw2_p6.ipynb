{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7949c03b",
   "metadata": {},
   "source": [
    "Change '../data/' to your local directory where the two folders 'easy_ham' and 'spam' are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "548610f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd46f31",
   "metadata": {},
   "source": [
    "Read email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c984d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def parse_email(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        msg = email.message_from_bytes(f.read())\n",
    "    # Get email headers\n",
    "    subject = msg.get('Subject')\n",
    "\n",
    "    # Read email's body\n",
    "    body = str(msg.get_payload())\n",
    "\n",
    "    # Remove HTML tags\n",
    "    body = BeautifulSoup(body).get_text()\n",
    "    \n",
    "    return subject, body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403231ea",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97790ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = str(text) # make sure input type is string.\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    text = text.lower() # Make lower case.\n",
    "    text = re.sub(r'=\\n', '', text)  # Remove =\\n.\n",
    "    text = re.sub(r'\\n', ' ', text)  # Remove \\n.\n",
    "    text = ' '.join(text.split())  # Replace all whitespaces to one space.\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        if token.like_url: # Check for URLs.\n",
    "            processed_tokens.append(\"urllink\") # Replace actual URL link to a word 'urllink'\n",
    "            continue\n",
    "        if token.like_email: # Check for email addresses.\n",
    "            processed_tokens.append(\"emailaddress\") # Replace actual email address to a word 'emailaddress'\n",
    "            continue\n",
    "        if not token.is_stop and not token.is_punct: # Check for stopwords and punctuations.\n",
    "            processed_tokens.append(token.lemma_) # Lemmatization.\n",
    "    text = \" \".join(processed_tokens)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Leave only alphabets and whitespaces.\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396f7d30",
   "metadata": {},
   "source": [
    "Preprocessing: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb6ff92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Add an additional filter for stopwords from TF-IDF.\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5e472",
   "metadata": {},
   "source": [
    "Pathlib to read files in data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c18ad3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dir_spam = Path(dir_data+\"spam\")\n",
    "dir_ham = Path(dir_data+\"easy_ham\")\n",
    "\n",
    "files_spam = list(dir_spam.iterdir())\n",
    "files_ham = list(dir_ham.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefe96c6",
   "metadata": {},
   "source": [
    "Test above functions on the first ham mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c815c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Preprocessed body:\n",
      "    url urllink date supply blake blogs mpt want mozilla look like msie admit evidence pretty compelling recall ask agree mpt  list quote list agree item mpt mention problem course people problem mind mpt agree problem necessarily agree solution problem maybe different idea solve particular issue believe issue need address cover list specifically navigator chrome structure necessarily agree mpt propose default configuration agree chrome structure painfully restrictive customizable toolbar need implement order acquire flexibility deal problem speed argue cut lot useless ui feature chrome help substantially reduce bloat gain speed text editing use chimera mac textfield widget easily painful entire application buggy slow misbehave edit way expect imo chimera usability problem message display yes argument search yeah mess know mess menu structure get blog app separate menu structure complicate order deal multiple application clean separation naturally simplify menu eg eliminate new submenu easily migration problem imo face mozilla context menus mpt complain click context menu os default win overwhelmingly bring context menu mouse like complain win cite mozilla usability problem follow convention operating system simply listen wmcontextmenu message message fire win want fire validation err usability problem average bear completely irrelevant preference imo high list preference tangled pathetic mess separate pref individual app unique dialog simplify thing great deal remove nearly half preference exist gui mozilla ridiculously overconfigurable  urllink  urllink\n",
      "*Feature names for TF-IDF:\n",
      " ['acquire' 'address' 'admit' 'agree' 'app' 'application' 'argue'\n",
      " 'argument' 'ask' 'average' 'bear' 'believe' 'blake' 'bloat' 'blog'\n",
      " 'blogs' 'bring' 'buggy' 'chimera' 'chrome' 'cite' 'clean' 'click'\n",
      " 'compelling' 'complain' 'completely' 'complicate' 'configuration'\n",
      " 'context' 'convention' 'course' 'cover' 'customizable' 'cut' 'date'\n",
      " 'deal' 'default' 'dialog' 'different' 'display' 'easily' 'edit' 'editing'\n",
      " 'eliminate' 'entire' 'err' 'evidence' 'exist' 'expect' 'face' 'feature'\n",
      " 'flexibility' 'follow' 'gain' 'great' 'gui' 'half' 'help' 'high' 'idea'\n",
      " 'imo' 'implement' 'individual' 'irrelevant' 'issue' 'item' 'know' 'like'\n",
      " 'list' 'listen' 'look' 'lot' 'mac' 'maybe' 'mention' 'menu' 'menus'\n",
      " 'mess' 'message' 'migration' 'mind' 'misbehave' 'mouse' 'mozilla' 'mpt'\n",
      " 'msie' 'multiple' 'naturally' 'navigator' 'nearly' 'necessarily' 'need'\n",
      " 'new' 'operating' 'order' 'os' 'overconfigurable' 'overwhelmingly'\n",
      " 'painful' 'painfully' 'particular' 'pathetic' 'people' 'pref'\n",
      " 'preference' 'pretty' 'problem' 'propose' 'quote' 'recall' 'reduce'\n",
      " 'remove' 'restrictive' 'ridiculously' 'search' 'separate' 'separation'\n",
      " 'simplify' 'simply' 'slow' 'solution' 'solve' 'specifically' 'speed'\n",
      " 'structure' 'submenu' 'substantially' 'supply' 'tangled' 'text'\n",
      " 'textfield' 'thing' 'toolbar' 'ui' 'unique' 'url' 'urllink' 'usability'\n",
      " 'use' 'useless' 'validation' 'want' 'way' 'widget' 'win' 'wmcontextmenu'\n",
      " 'yeah' 'yes']\n",
      "*Shape of TF-IDF:\n",
      "      (1, 148)\n"
     ]
    }
   ],
   "source": [
    "subject, body = parse_email(files_ham[0])\n",
    "body_preprocess = preprocess(body)\n",
    "body_tfidf = vectorizer.fit_transform([body_preprocess])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# print(body)\n",
    "print(f\"*Preprocessed body:\\n    {body_preprocess}\")\n",
    "print(f\"*Feature names for TF-IDF:\\n {feature_names}\")\n",
    "print(f\"*Shape of TF-IDF:\\n      {body_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342fce3b",
   "metadata": {},
   "source": [
    "Create a dataframe with preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6966cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# label = [0] * len(files_ham)\n",
    "# label.append([1] * len(files_spam))\n",
    "\n",
    "subject = []\n",
    "body = []\n",
    "label = []\n",
    "\n",
    "for file in files_ham:\n",
    "    subject_ham, body_ham = parse_email(file)\n",
    "    subject.append(preprocess(subject_ham))\n",
    "    body.append(preprocess(body_ham))\n",
    "    label.append(0)\n",
    "\n",
    "for file in files_spam:\n",
    "    subject_spam, body_spam = parse_email(file)\n",
    "    subject.append(preprocess(subject_spam))\n",
    "    body.append(preprocess(body_spam))\n",
    "    label.append(1)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        'subject': subject,\n",
    "        'body': body,\n",
    "        \n",
    "        'target': label\n",
    "    }\n",
    ")\n",
    "df.to_csv(dir_data+'data_spam.csv', index=False) # Load csv file instead of running preprocessing again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7110de1b",
   "metadata": {},
   "source": [
    "Above cell requires about 24 minutes on preprocessing, so I recommend directly using the preprocessed csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f72b68a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usability problem mozilla</td>\n",
       "      <td>url urllink date supply blake blogs mpt want m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exmh speed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hanson sept  message national review</td>\n",
       "      <td>hanson good sci fi author plan slip follow lin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>use new apt null rh upgrade</td>\n",
       "      <td>matthias saou emailaddress write  red hat reco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>satalk funny</td>\n",
       "      <td>wednesday  august   cet theo van dinter write ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>low cost easy use conference</td>\n",
       "      <td>low rate service conference easy  cent minute ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>shape summer</td>\n",
       "      <td>see nbc cbs cnn oprah health discovery actuall...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>american millionaire reveal secret source weal...</td>\n",
       "      <td>hello                                         ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>gain low interest rate  year</td>\n",
       "      <td>opportunity knock mortgage rate rise national ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>fw short term buy recommendation</td>\n",
       "      <td>otc newsletter discover tomorrow winner immedi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3001 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                subject  \\\n",
       "0                             usability problem mozilla   \n",
       "1                                            exmh speed   \n",
       "2                  hanson sept  message national review   \n",
       "3                           use new apt null rh upgrade   \n",
       "4                                          satalk funny   \n",
       "...                                                 ...   \n",
       "2996                       low cost easy use conference   \n",
       "2997                                       shape summer   \n",
       "2998  american millionaire reveal secret source weal...   \n",
       "2999                       gain low interest rate  year   \n",
       "3000                   fw short term buy recommendation   \n",
       "\n",
       "                                                   body  target  \n",
       "0     url urllink date supply blake blogs mpt want m...       0  \n",
       "1                                                   NaN       0  \n",
       "2     hanson good sci fi author plan slip follow lin...       0  \n",
       "3     matthias saou emailaddress write  red hat reco...       0  \n",
       "4     wednesday  august   cet theo van dinter write ...       0  \n",
       "...                                                 ...     ...  \n",
       "2996  low rate service conference easy  cent minute ...       1  \n",
       "2997  see nbc cbs cnn oprah health discovery actuall...       1  \n",
       "2998  hello                                         ...       1  \n",
       "2999  opportunity knock mortgage rate rise national ...       1  \n",
       "3000  otc newsletter discover tomorrow winner immedi...       1  \n",
       "\n",
       "[3001 rows x 3 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_csv(dir_data+\"data_spam.csv\")\n",
    "df.head(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds5220",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
